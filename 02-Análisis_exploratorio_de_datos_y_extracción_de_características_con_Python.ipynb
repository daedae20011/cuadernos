{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Análisis exploratorio de datos y extracción de características con Python\n\nUsando **visualización de datos**, **ingeniería de funciones** y **selección de funciones** para hacer que una simple **regresión logística** parezca poderosa.","metadata":{"_cell_guid":"2ed88b1f-380e-4b02-a694-04f003453e65","_uuid":"0b05f17541802c7a3736f5f8e306d32dbeb7c9b6"}},{"cell_type":"markdown","source":"Yo era un niño cuando se estrenó la película 'Titanic'. En mi mente aún guardo una imagen de toda esa gente, afuera del cine, en la fila para comprar boletos. En ese entonces, Leonardo DiCaprio y Kate Winslet eran solo dos niños pequeños con lindos cortes de cabello y los boletos en línea eran ciencia ficción.\n​\nSegún los ingenieros, clase a la que orgullosamente pertenezco, el Titanic era el barco insumergible. Era hermoso, lujoso y equipado con lo mejor de la tecnología. Titanic fue el crucero de última generación. Simplemente no era tan insumergible.\n​\nDespués de 100 años, el Titanic sigue siendo tema de discusión en los más diversos ámbitos. Por ejemplo, puede encontrar [libros](https://amzn.to/2Gie0Pv) donde el autor toma lecciones de liderazgo del Titanic para aplicar en los negocios; puede encontrar [proyectos de IA] interesantes (http://fortune.com/2018/03/26/china-titanic-artificial-intelligence-sensetime/) que aplican el aprendizaje profundo para distinguir las escenas románticas del Titanic de las escenas de desastre; o puede encontrar extensos ejercicios de pensamiento creativo sobre [lo que realmente sucedió con el Titanic](https://www.bustle.com/p/6-titanic-conspiracy-theories-that-are-still-fascinating-today-28519 ).\n​\nTambién usaremos Titanic para un propósito específico: aprender técnicas de **análisis exploratorio de datos** y **extracción de características**. A través de un análisis completo del problema del Titanic de Kaggle, veremos qué hacer, por dónde empezar y cómo proceder en un problema de ciencia de datos. Se abordarán temas como **visualización de datos**, **imputación de datos faltantes**, **ingeniería de funciones**, **selección de funciones** y **regresión logística**, que le servirán repetidamente porque después de ver lo que involucrado, podrá aplicar estas técnicas a cualquier tipo de problema de ciencia de datos.\n​\nDado que soy un verdadero creyente del poder transformador de las empresas emergentes y veo similitudes obvias entre lo que hacen las empresas emergentes y lo que hace un científico de datos, varias referencias a los métodos de empresas emergentes están presentes en el texto. Además, encontrarás algunos chistes sobre el Titanic. No es que crea en su poder para hacer reír a alguien, pero solo porque son geniales para romper el hielo...\n​\nEste núcleo se ha dividido en cuatro partes. La primera parte trata sobre el desarrollo de un modelo de referencia. Este modelo debería permitirnos comprender rápidamente el problema y los datos. Después, entraremos en detalles. Los datos se estudiarán y enriquecerán a través del análisis exploratorio de datos y la extracción de características, para mejorar el rendimiento de nuestro modelo de aprendizaje automático. Finalmente, se extraerán algunas conclusiones de este kernel y su impacto en nuestro viaje de ciencia de datos.\n​\n**Índice**\n​\n1. [El conjunto de datos lean](#1.-The-lean-data-set)\n​\n  1.1. [Hacer el lanzamiento] (#1.1.-Hacer el lanzamiento)\n \n  1.2. [Mostrando los números](#1.2.-Mostrando-los-números)\n \n  1.3. [Rellenando los huecos](#1.3.-Rellenando-los-huecos)\n \n  1.4. [Modelo mínimo viable](#1.4.-Modelo-mínimo-viable)\n \n2. [El conjunto de datos gorditos](#2.-El-conjunto-de-datos-gorditos)\n \n  2.1. [Imputación de datos faltantes de 'Edad'](#2.1.-Imputación-de-datos-faltantes-de-'Edad')\n \n  2.2. [Análisis exploratorio de datos](#2.2.-Análisis-exploratorio-de-datos)\n \n  2.3. [Extracción de características](#2.3.-Extracción de características)\n \n3. [Modelo Unicornio](#3.-Modelo Unicornio)\n​\n  3.1. [Ajustar modelo para la mejor combinación de características](#3.1.-Ajustar-modelo-para-la-mejor-combinación-de-características)\n \n  3.2. [Curva de aprendizaje](#3.2.-Curva-de-aprendizaje)\n \n  3.3. [Curva de validación](#3.3.-Curva-de-validación)\n \n  3.4. [Enviar predicciones](#3.4.-Enviar-predicciones)\n \n4. [Conclusión](#4.-Conclusión)\n​\n**Advertencia:** Esta será una lectura larga. Prepararse. Traiga su cuaderno, siéntese en su silla favorita y vierta Cola en un vaso lleno de hielo. El hielo es importante porque la cola siempre va bien con hielo. Igual que el Titanic (te lo dije).\n​\n---","metadata":{"_cell_guid":"8f575fa1-5db4-46f5-86cb-d84b9194a117","_uuid":"16e35ed33ca4dfb9fb21138f5bbe0398bc5b971f"}},{"cell_type":"markdown","source":"#0. Belfast, una incubadora anterior","metadata":{"_cell_guid":"543674f5-aaf8-47fb-a17c-668a89c6a7f7","_uuid":"62d9979ba27bc6c2ed36deed1a0ea0a0bdb68f76"}},{"cell_type":"markdown","source":"Las incubadoras son empresas que apoyan la creación de startups y sus primeros años de actividad. Son importantes porque ayudan a los empresarios a resolver algunos problemas comúnmente asociados con la gestión de un negocio, como el espacio de trabajo, la capacitación y la financiación inicial.\n\nNuestra obra maestra de ingeniería también necesita un punto de partida. En esta sección, comenzamos el ensamblaje de nuestro trabajo importando algunas bibliotecas y funciones generales.","metadata":{"_cell_guid":"c6bee3e3-3756-40ed-8516-1887b494794a","_uuid":"94ab674f2a935da4aab5cd0dbf2086d1b6c12ac1"}},{"cell_type":"markdown","source":"## Imports","metadata":{"_cell_guid":"7ea5a751-3bc7-40bb-a789-d01d87ab2f12","_uuid":"4454f0dc3023c06875737d99e12d9ba40c3d7813"}},{"cell_type":"code","source":"# Import librerias\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Pon esto cuando se llame\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression","metadata":{"_cell_guid":"24eda48f-f136-40ff-9bec-27ba612cce6b","_uuid":"1222390a7e9d88443a5d8d8c0489292e9f052e61","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions","metadata":{"_cell_guid":"dd306186-a9b2-4e62-bf1a-a40535411212","_uuid":"22332c7b42cc1783e12fbe1a0f957224181083e6"}},{"cell_type":"code","source":"# Crear tabla para análisis de datos faltantes\ndef draw_missing_data_table(df):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data","metadata":{"_cell_guid":"7d098a03-2b97-41ce-9bea-eb7973830021","_uuid":"a59bd292c30d8d1847573f79a869c5df6b4bca55","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Trazar la curva de aprendizaje\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","metadata":{"_cell_guid":"842d0491-6d3b-4269-ad14-669af85e7b35","_uuid":"2a8bbef09541ed931997ab12fe511832d20d788e","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Trazar la curva de validación\ndef plot_validation_curve(estimator, title, X, y, param_name, param_range, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    train_scores, test_scores = validation_curve(estimator, X, y, param_name, param_range, cv)\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    plt.plot(param_range, train_mean, color='r', marker='o', markersize=5, label='Training score')\n    plt.fill_between(param_range, train_mean + train_std, train_mean - train_std, alpha=0.15, color='r')\n    plt.plot(param_range, test_mean, color='g', linestyle='--', marker='s', markersize=5, label='Validation score')\n    plt.fill_between(param_range, test_mean + test_std, test_mean - test_std, alpha=0.15, color='g')\n    plt.grid() \n    plt.xscale('log')\n    plt.legend(loc='best') \n    plt.xlabel('Parameter') \n    plt.ylabel('Score') \n    plt.ylim(ylim)","metadata":{"_cell_guid":"f03c3d33-d71f-41f5-909e-e35f3b18b2de","_uuid":"605f7f90ae0f19cc61fb2f46846a483983f63ebf","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{"_cell_guid":"7cd18981-748d-4a44-862e-d012970223ff","_uuid":"59a9cb76453cd6b6b73710b1285342209906226f"}},{"cell_type":"markdown","source":"# 1. El conjunto de datos de aprendizaje","metadata":{"_cell_guid":"bea8dcd8-6bb9-48ab-afe8-2106e0d3b718","_uuid":"6a99d1c90455c0502b3f9dcc41413389246aa813"}},{"cell_type":"markdown","source":"En el libro ['The Lean Startup'](https://amzn.to/2sHpnvP), Eric Ries nos cuenta sus experiencias personales adaptando los principios de gestión lean a empresas emergentes de alta tecnología. A través de una serie de anécdotas e historias, Ries nos enseña todo lo que debemos saber sobre la agilidad y la metodología lean en el mundo de las startups.\n\nSi bien se enseña un conjunto de principios importantes a lo largo del libro, la verdad es que la metodología lean startup siempre termina en un intento de responder a la pregunta: '¿Debe construirse este producto?'\n\nPara responder a esta pregunta, el enfoque lean startup se basa en un proceso Construir-Medir-Aprender. Este proceso enfatiza la iteración rápida como un ingrediente crítico para el desarrollo de productos. Pasa por las siguientes fases:\n1. **Construir**. Averigüe el problema que debe resolverse, genere ideas sobre cómo resolverlo y seleccione la mejor. Convierte tu mejor idea en un Producto Mínimo Viable (MVP).\n2. **Medir**. Pruebe su producto. Dirígete a tus clientes y mide sus reacciones y comportamientos frente a tu producto.\n3. **Aprender**. Analice los datos que recopiló al probar el producto con sus clientes. Sacar conclusiones del experimento y decidir qué hacer a continuación.\nEn otras palabras, este es un proceso de aprendizaje validado que crea, prueba y reconstruye productos rápidamente, de acuerdo con los comentarios de los usuarios. Esto reduce sus riesgos de mercado al fallar rápido y barato, para acercarlo más y más a lo que el mercado realmente necesita.\n\nEste núcleo hace algo similar. Intentaremos fallar de forma rápida y económica construyendo rápidamente una canalización de extremo a extremo que funcione (Build). Luego, instrumentaremos el sistema para evaluar su desempeño (Medir). Finalmente, haremos cambios incrementales para mejorar el rendimiento del sistema (Learn). Tenga en cuenta que esta metodología práctica fue adaptada de Goodfellow et al. (2016), un libro al que puede acceder de forma gratuita [aquí] (http://www.deeplearningbook.org/).\n\nInicialmente, no invertiremos mucho tiempo en el análisis exploratorio de datos. Simplemente haremos el mínimo esfuerzo viable para implementar un modelo razonable. Este modelo será nuestro 'Modelo Mínimo Viable'. Más adelante, intentaremos superar este modelo enriqueciendo nuestros datos.\n\n**Pregunta sorpresa:** ¿fue el Titanic un MVP?","metadata":{"_cell_guid":"7fd25c6d-d20c-4d68-b283-ee6cf4d3313f","_uuid":"68c1fb7bcf60ac0a5dd7931e1c528ad1df7fb207"}},{"cell_type":"markdown","source":"## 1.1. haciendo el lanzamiento","metadata":{"_cell_guid":"0b6446e6-2c25-476f-97a9-fb4ed0d78535","_uuid":"1342848d65a2cd7a2334e2b6545398c94ac0bbd6"}},{"cell_type":"markdown","source":"Las startups usan lanzamientos para vender su idea. En consecuencia, su presentación debe ser clara y concisa, respondiendo a preguntas como '¿qué haces?', '¿qué quieres?' y '¿quién está en tu equipo?'. El tono es importante porque los inversores están más dispuestos a invertir cuando entienden lo que estás haciendo.\n\nDevolvamos las primeras filas de nuestro conjunto de datos para obtener una imagen clara y concisa de lo que hay allí y lo que podemos hacer con él.","metadata":{"_cell_guid":"6a356067-9714-4b3f-87c0-2423d749e401","_uuid":"88b59df036f54ab10297394bdb3344f48c700ed5"}},{"cell_type":"code","source":"# Importando datos\ndf = pd.read_csv('../input/train.csv')\ndf_raw = df.copy()  # Save original data set, just in case.","metadata":{"_cell_guid":"5af9f619-5892-45b7-a75b-8402db274527","_uuid":"f54e57ad0a4c79f81b5c8049ae3a54d82d6b71d4","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Descripción general\ndf.head()","metadata":{"_cell_guid":"d6251fb6-efb6-4659-b6ff-577e8d489374","_uuid":"cbdb3b1ff3273b7f0f8159fc038390057d5c9a2d","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Definiciones y pensamientos rápidos:\n\n* **Id del Pasajero**. Identificación única del pasajero. No debería ser necesario para el modelo de aprendizaje automático.\n* **Sobrevivió**. Supervivencia (0 = No, 1 = Sí). Variable binaria que será nuestra variable objetivo.\n* **Clase P**. Clase de boleto (1 = 1°, 2 = 2°, 3 = 3°). Listo para ir.\n* **Nombre**. Nombre del pasajero. Necesitamos analizar antes de usarlo.\n* **Sexo**. Sexo. Variable categórica que debe [codificarse] (http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features).\n* **Edad**. Edad en años. Listo para ir.\n* **Esp.Sib**. # de hermanos/cónyuges a bordo del Titanic. Listo para ir.\n* **Parque**. # de padres/hijos a bordo del Titanic. Listo para ir.\n* **Boleto**. Numero de ticket. Gran desorden. Necesitamos entender su estructura primero.\n* **Tarifa**. Tarifa de pasajero. Listo para ir.\n* **Cabina**. Número de cabina. Necesita ser analizado.\n* **Embarcado**. Puerto de embarque (C = Cherburgo, Q = Queenstown, S = Southampton). Rasgo categórico que debe ser codificado.\n\nLa conclusión principal es que ya tenemos un conjunto de características que podemos usar fácilmente en nuestro modelo de aprendizaje automático. Otras características, como 'Nombre', 'Boleto' y 'Tarifa', requieren un esfuerzo adicional antes de que podamos integrarlas.","metadata":{"_cell_guid":"497872d5-6e5d-49b8-af2e-a5541f865e5a","_uuid":"0ae55aabfaf461bc58be6b04e124f80bd674a805"}},{"cell_type":"markdown","source":"## 1.2. Mostrando los números","metadata":{"_cell_guid":"1d128c91-6b56-4674-994e-2f36f8056e70","_uuid":"fad001a83d87c292d396064cd26a8c6b6274a236"}},{"cell_type":"markdown","source":"Los números son cruciales para establecer objetivos, tomar decisiones comerciales acertadas y obtener dinero de los inversores. Con números puedes proyectar el futuro de tu startup, para que todos puedan entender cuáles son las expectativas en torno a tu idea.\n\nDel mismo modo, generaremos las estadísticas descriptivas para obtener la información cuantitativa básica sobre las características de nuestro conjunto de datos.","metadata":{"_cell_guid":"48fae33f-abc7-4c73-af75-8c857fab1901","_uuid":"6b1e7c2a4e524914d5a63cc28d9664637650aafd"}},{"cell_type":"code","source":"# Estadísticas descriptivas\ndf.describe()","metadata":{"_cell_guid":"9c5d5bdc-d6b7-4b2f-98c8-62673846ec06","_uuid":"c85c2be0d128e0bf64d330cfa1c43f721ea51ffd","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hay tres aspectos que suelen llamar mi atención cuando analizo estadísticas descriptivas:\n1. **Valores mínimos y máximos**. Esto puede darnos una idea sobre el rango de valores y es útil para detectar valores atípicos. En nuestro caso, todos los valores mínimos y máximos parecen razonables y dentro de un rango razonable de valores. La única excepción podría ser el valor máximo de 'Tarifa', pero por ahora lo dejaremos como está.\n2. **Media y desviación estándar**. La media nos muestra la tendencia central de la distribución, mientras que la desviación estándar cuantifica su cantidad de variación. Por ejemplo, una desviación estándar baja sugiere que los puntos de datos tienden a estar cerca de la media. Dando un vistazo rápido a nuestros valores, no hay nada que parezca obviamente incorrecto.\n3. **Contar**. Esto es importante para darnos una primera percepción sobre el volumen de datos faltantes. Aquí, podemos ver que faltan algunos datos de 'Edad'.\n\nDado que no hay nada impactante en las variables, pasemos al siguiente paso: datos faltantes.","metadata":{"_cell_guid":"d6a15f54-8e0f-4fea-afcb-5941fbaaedad","_uuid":"8cd71bba96b1769b22157cee1a84003063593eed"}},{"cell_type":"markdown","source":"## 1.3. llenando los huecos","metadata":{"_cell_guid":"74aa736b-1254-4f60-95a7-da10a3bfbdd7","_uuid":"099f68e75192e6f96d14deb616b02de8966c6e07"}},{"cell_type":"markdown","source":"Una de mis definiciones favoritas de startup pertenece a Eric Ries: 'una startup es una institución humana diseñada para crear un nuevo producto o servicio en condiciones de extrema incertidumbre'.\n\nLa palabra 'incertidumbre' es clave en esta definición y también es clave en la falta de datos. Los datos faltantes ocurren cuando no hay disponible ningún valor de datos en una o más variables. En consecuencia, reduce el tamaño del conjunto de datos y es una posible fuente de sesgo, ya que algún mecanismo no aleatorio puede estar generando los datos que faltan. Como resultado, la falta de datos introduce incertidumbre en nuestro análisis.\n\nExisten varias estrategias para lidiar con los datos faltantes. Algunos de los más comunes son:\n* Utilizar solo datos válidos, eliminando los casos en los que falten datos.\n* Imputa datos usando valores de casos similares o usando el valor medio.\n* Imputar datos utilizando métodos basados en modelos, en los que se definen modelos para predecir los valores que faltan.\n\nHasta hoy, nunca he encontrado una solución de \"talla única\". Tengo algunos dogmas (por ejemplo, suelo excluir variables con más del 25% de datos faltantes), pero lo que suele guiar mi análisis es la intuición, el pensamiento crítico y la necesidad (a veces necesitamos dejar nuestros dogmas en la puerta, si queremos generar algunos resultados).\n\nMi consejo práctico para manejar los datos faltantes es aprender un conjunto diferente de herramientas. Juega con ellos según tus necesidades, pruébalos y deberías estar bien. Una buena introducción al tema se puede encontrar en [Hair et al. (2013)](https://amzn.to/2M9v0uW). Este libro tiene un resumen práctico sobre los datos que faltan y proporciona un marco que puede aplicar en casi todas las situaciones. Además, escribí un documento técnico comparando diferentes técnicas de imputación, que puedo compartir con usted si lo desea.\n\nAhora que podemos ver la punta del iceberg, profundicemos en el tema.","metadata":{"_cell_guid":"2f8e3feb-8ac9-4fe9-8434-2481c6d0bcb0","_uuid":"bb694a1ba0d01b026588d5dce1d0ffecbd145021"}},{"cell_type":"code","source":"# Analizar datos faltantes\ndraw_missing_data_table(df)","metadata":{"_cell_guid":"ee32f22a-c620-449c-975c-53b7cfe5900f","_uuid":"d31401ab943a22b62e3c9de19735e1d9bb8536c9","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Primeros pensamientos:\n\n* 'Cabina' tiene demasiados valores faltantes (>25%). ¡Dogma! Necesitamos eliminar esta variable de inmediato.\n* Se puede imputar 'Edad'. Por ahora, asociaré un valor que me permita saber que estoy imputando datos. Más tarde, revisaré esta estrategia.\n* Debido al bajo porcentaje de valores faltantes, eliminaré las observaciones donde no sabemos 'Embarcado'.","metadata":{"_cell_guid":"e1064e09-c8bf-42da-82da-ba55918a01a2","_uuid":"2af9ecbab206a2fe3aa1aaa0b87d0e6c3aaafff7"}},{"cell_type":"code","source":"# Caída de cabina\ndf.drop('Cabin', axis=1, inplace=True)\ndf.head()","metadata":{"_cell_guid":"e8af5f35-6d52-478e-92e8-fa7491da1cd3","_uuid":"7f08c40f910df8565fb7d247c905d315e6f52ac4","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rellene los valores faltantes en Edad con un valor específico\nvalue = 1000\ndf['Age'].fillna(1000, inplace=True)\ndf['Age'].max()","metadata":{"_cell_guid":"263da52c-1d79-43bb-a5d2-40fbedac791d","_uuid":"10d5478459e6cfe6472ccb6d25b47763b63026b1","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Borrar observaciones sin Embarcado\ndf.drop(df[pd.isnull(df['Embarked'])].index, inplace=True)  # Obtener índice de puntos donde Embarked es nulo\ndf[pd.isnull(df['Embarked'])]","metadata":{"_cell_guid":"423559d6-fe20-47de-a75c-a8b906f278a5","_uuid":"f7a0d23428db9bd6322f031785da93fa3c63c98a","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.4. Modelo mínimo viable","metadata":{"_cell_guid":"4034a188-7273-40f0-bb4e-9ce02560333d","_uuid":"31b68a30e8e4f64a06fa099a6ded19fd3b3eee2b"}},{"cell_type":"markdown","source":"El 'Producto Mínimo Viable' (MVP) es un concepto clave para cualquier startup lean. Una vez que se resuelve el problema a resolver, el enfoque de la puesta en marcha debe estar en el desarrollo de una solución, el MVP, lo más rápido posible. Gracias al MVP, es posible iniciar el proceso de aprendizaje y mejorar la solución hacia las necesidades de los usuarios.\n\n[Goodfellow et al. (2016)](http://www.deeplearningbook.org/contents/guidelines.html) propone un enfoque análogo para la aplicación de modelos de aprendizaje automático. Como señalan los autores, la aplicación exitosa de técnicas de aprendizaje automático va más allá del conocimiento de los algoritmos y sus principios. Para aplicar con éxito las técnicas de aprendizaje automático, debemos comenzar con un modelo simple que podamos dominar y comprender. Solo entonces deberíamos pasar a algoritmos más complejos.\n\nLos autores proponen una metodología práctica de cuatro pasos:\n1. Seleccione una métrica de rendimiento y un valor objetivo para esta métrica. Esta métrica guiará su trabajo y le permitirá saber qué tan bien se está desempeñando. En nuestro caso, nuestra métrica de rendimiento será la \"precisión\" porque es la definida por [Kaggle](https://www.kaggle.com/c/titanic#e Evaluation).\n2. Configure rápidamente una canalización integral que funcione. Esto debería permitirle estimar la métrica de rendimiento seleccionada.\n3. Supervisar el sistema para comprender su comportamiento, en particular para comprender si su bajo rendimiento está relacionado con un ajuste insuficiente, un ajuste excesivo o defectos.\n4. Mejorar el sistema por iteración. Aquí podemos aplicar ingeniería de características, ajustar hiperparámetros o incluso cambiar el algoritmo, de acuerdo con las salidas de nuestro sistema de monitoreo.\n\nSeguiremos esta metodología. En consecuencia, nuestro objetivo será obtener un modelo inicial que podamos utilizar como un primer enfoque de referencia. Este modelo será nuestro 'Modelo Mínimo Viable' (MVM). Tenga en cuenta que en este momento no importa mucho qué tan bien se desempeñe el modelo. Sólo necesitamos un punto de partida. En definitiva, somos empresarios. En el peor de los casos, llamamos a este modelo 'versión beta' :P\n\nBien, preparemos los datos para el lanzamiento de MVM, ajustemos una regresión logística y analicemos el rendimiento del modelo a través de [curvas de aprendizaje y validación] (http://scikit-learn.org/stable/modules/learning_curve.html ).","metadata":{"_cell_guid":"6b076d84-98ef-45ce-8b82-d3075d568f67","_uuid":"29154265d94ed9ddff7c94c24827c1bfce5a2c47"}},{"cell_type":"markdown","source":"### 1.4.1. Preparando los datos","metadata":{"_cell_guid":"39627508-4f51-4197-9085-2cc696894cd7","_uuid":"c6d91fc0cdde9eb1e230c38390f5ddf9127808bb"}},{"cell_type":"code","source":"# Tipos de datos\ndf.dtypes","metadata":{"_cell_guid":"8b022c48-64e7-4e99-bde9-21d0bb738f99","_uuid":"09d36014314a1c9d0612e89dc538ae32042e22a2","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* No necesitamos 'PassengerId' para fines de predicción, por lo que lo excluiremos.\n* 'Sexo', 'Embarcado' y 'Pclase' deben ser categóricos. No consideraré 'Sobrevivido' como categórico porque es la variable de salida.\n* Necesitamos analizar 'Nombre' y 'Boleto'. Por ahora, ignoraré estas características.\n* 'SibSp' podría agruparse con 'Parch' para crear una característica de 'Familia'. Por ahora solo identificaré si el pasajero viaja solo o con su familia.","metadata":{"_cell_guid":"b7b5679f-a895-42d0-b8d7-95b6f56f86b5","_uuid":"42ee64ebbc93c98b997a860913776c92a7f10551"}},{"cell_type":"code","source":"# Soltar ID de pasajero\ndf.drop('PassengerId', axis=1, inplace=True)\ndf.head()","metadata":{"_cell_guid":"b56d95ea-a955-4ec1-8545-3aae3049c806","_uuid":"79b2b437911018acce12975612bb8bce64c8a535","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Definir variables categóricas\ndf['Sex'] = pd.Categorical(df['Sex'])\ndf['Embarked'] = pd.Categorical(df['Embarked'])","metadata":{"_cell_guid":"fbef3502-3471-49b9-bddc-497965b017b7","_uuid":"b94f44f7713d046560c0242b08550de0bee02366","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crear función familiar\ndf['FamilySize'] = df['SibSp'] + df['Parch']\ndf.head()","metadata":{"_cell_guid":"83859041-24f4-4fcc-9ede-b046313c9e9d","_uuid":"82f2d8f11adfba53636be799e1ca3210a5e8d247","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Soltar SibSp y Parch\ndf.drop('SibSp',axis=1,inplace=True)\ndf.drop('Parch',axis=1,inplace=True)\ndf.head()","metadata":{"_cell_guid":"686b8e25-cad9-4e11-ba54-ca2daac1fc94","_uuid":"33700ab050e3f52b5d03577a28c54c0c511499dc","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Soltar Nombre y Ticket\ndf.drop('Name', axis=1, inplace=True)\ndf.drop('Ticket', axis=1, inplace=True)\ndf.head()","metadata":{"_cell_guid":"9cb9b91c-1a6f-49ab-b77a-1d24df6ad03e","_uuid":"89bb69b267c2009626f029a7ddfc5a009eb99ce0","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.4.2. Lanzamiento del modelo","metadata":{"_cell_guid":"a830954a-a349-482b-aaf0-40a35a1b44e8","_uuid":"c9502638180cb906dd3a9e22d87cbbc220b023b0"}},{"cell_type":"markdown","source":"Ready... Set... [Go!](https://youtu.be/_YzNZE287QQ)","metadata":{"_cell_guid":"ab8ede75-513e-4ba5-b837-57c6e5dc1f39","_uuid":"ac6783003b0024dc984a2384bde95c555674d51b"}},{"cell_type":"code","source":"# Transformar variables categóricas en variables ficticias\ndf = pd.get_dummies(df, drop_first=True) # Para evitar la trampa ficticia\ndf.head()","metadata":{"_cell_guid":"9ab0f1c5-64d2-4621-8c5c-6943e73c9449","_uuid":"181d8c10cc1657f8c024796288f9596c98a032ac","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Extra: [¿Qué es la 'trampa de variable ficticia'?](http://www.algosome.com/articles/dummy-variable-trap-regression.html)","metadata":{"_cell_guid":"28752da9-accf-46e4-bc07-5fb57ab817da","_uuid":"820ab4de5c972fef9f78a098be7262d2d75fc698"}},{"cell_type":"code","source":"# Crear un conjunto de datos para entrenar métodos de imputación de datos\nX = df[df.loc[:, df.columns != 'Survived'].columns]\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)","metadata":{"_cell_guid":"97de3798-60bb-4abe-93b5-a35251f078c1","_uuid":"fa31f4467d79d93602f93edd36699c5ec9768c3b","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Debug\nprint('Inputs: \\n', X_train.head())\nprint('Outputs: \\n', y_train.head())","metadata":{"_cell_guid":"9ee8569e-9691-4b69-9533-9520e7ac3bba","_uuid":"41c62a2953fc15413e6b0105e7d49d436ffe60f9","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ajustar regresión logística\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)","metadata":{"_cell_guid":"69b509c9-03e2-424a-808f-9e9f28ff46af","_uuid":"37f0fb6f93fa855b8bcb34025709d0e9a1840dee","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rendimiento del modelo\nscores = cross_val_score(logreg, X_train, y_train, cv=10)\nprint('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))","metadata":{"_cell_guid":"76111680-eafc-4972-be4c-68457e350b3f","_uuid":"e43d1cca08c5022f8319aa5f4f48347d0e3ea1c6","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.4.3.  Assessing model performance","metadata":{"_cell_guid":"b6a4b442-19e5-4e2f-a986-aa797c2341c7","_uuid":"1626cc74ca408a382afe58dcaf278922820ef493"}},{"cell_type":"code","source":"# Trazar curvas de aprendizaje\ntitle = \"Learning Curves (Logistic Regression)\"\ncv = 10\nplot_learning_curve(logreg, title, X_train, y_train, ylim=(0.7, 1.01), cv=cv, n_jobs=1);","metadata":{"_cell_guid":"b5a87776-c04a-43bd-a2f4-b600f444308e","_uuid":"a7aefbeb3b1f7fc687bdd15dd222e30e9bb44831","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Curvas de aprendizaje en pocas palabras:\n* Las curvas de aprendizaje nos permiten diagnosticar si el **sobreajuste** o **falta de ajuste**.\n* Cuando el modelo **sobreajusta**, significa que funciona bien en el conjunto de entrenamiento, pero no en el conjunto de validación. En consecuencia, el modelo no es capaz de generalizar a datos no vistos. Si el modelo se sobreajusta, la curva de aprendizaje presentará una brecha entre los puntajes de entrenamiento y validación. Dos soluciones comunes para el sobreajuste son reducir la complejidad del modelo y/o recopilar más datos.\n* Por otro lado, el **ajuste insuficiente** significa que el modelo no puede funcionar bien en los conjuntos de entrenamiento o validación. En esos casos, las curvas de aprendizaje convergerán a un valor de puntaje bajo. Cuando el modelo no se ajusta bien, la recopilación de más datos no es útil porque el modelo ya no puede aprender los datos de entrenamiento. Por lo tanto, los mejores enfoques para estos casos son mejorar el modelo (p. ej., ajustar los hiperparámetros) o mejorar la calidad de los datos (p. ej., recopilar un conjunto diferente de características).\n​\nDiscusión de nuestros resultados:\n* El modelo no se sobreencaja. Como podemos ver, las curvas convergen y no existe brecha entre el entrenamiento y la puntuación de validación en los últimos puntos de la curva.\n* El modelo se desadapta. Nuestra puntuación final es de aproximadamente 0,786. Aunque nuestro modelo hace mejores predicciones que una [estrategia de lanzar una moneda] (https://en.wikipedia.org/wiki/Flipism), todavía está lejos de ser un modelo 'inteligente'. Por ahora, es solo un modelo 'artificial'.","metadata":{"_cell_guid":"a541f404-488e-4769-b1d2-a1472b681c82","_uuid":"0a8ef3e423e47618342e29893f10702b8c550924"}},{"cell_type":"code","source":"# Trazar la curva de validación\ntitle = 'Validation Curve (Logistic Regression)'\nparam_name = 'C'\nparam_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0] \ncv = 10\nplot_validation_curve(estimator=logreg, title=title, X=X_train, y=y_train, param_name=param_name,\n                      ylim=(0.5, 1.01), param_range=param_range);","metadata":{"_cell_guid":"4fa2c196-3f31-4cbd-894a-de53df3b3962","_uuid":"2ef627a383f08b6eed66a8b5371e1c92e4b1a1d5","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Curvas de validación en pocas palabras:\n* Las curvas de validación son una herramienta que podemos utilizar para mejorar el rendimiento de nuestro modelo. Cuenta como una forma de ajustar nuestros hiperparámetros.\n* Son diferentes a las curvas de aprendizaje. Aquí, el objetivo es ver cómo el parámetro del modelo afecta los puntajes de capacitación y validación. Esto nos permite elegir un valor diferente para el parámetro, para mejorar el modelo.\n* Una vez más, si hay una brecha entre el entrenamiento y el puntaje de validación, es probable que el modelo esté sobreajustado. Por el contrario, si no hay brecha pero el valor de la puntuación es bajo, podemos decir que el modelo no se ajusta.\n\nDiscusión de nuestros resultados:\n* La figura muestra que no hay una gran diferencia en el rendimiento del modelo en la medida en que elegimos un valor C de $10^{-1}$ o superior. Tenga en cuenta que en una regresión logística, C es el único parámetro del modelo que podemos cambiar [(consulte la documentación de scikit-learn)](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression). ","metadata":{"_cell_guid":"f562059d-80d6-4930-8983-fd6c44534c64","_uuid":"c666b2a4b6bbe7a8f130ed54c984069e2f218303"}},{"cell_type":"markdown","source":"---","metadata":{"_cell_guid":"a325f4fa-34f3-44ef-9401-3e2def89723b","_uuid":"403af5a9f228e7caba9d4418edaadd4f21f5b761"}},{"cell_type":"markdown","source":"# 2. El conjunto de datos gordito","metadata":{"_cell_guid":"c695e1eb-8a7b-4bba-bc3a-caba0cc1f9d5","_uuid":"2a512172ed2746b1e7ec1598ee3ead43b574a7a4"}},{"cell_type":"markdown","source":"En este punto, nuestro modelo:\n\n* Puede lograr una precisión de 0,786 +/- 0,026.\n* Se basa en una regresión logística.\n* Utiliza 'Pclass', 'Edad', 'Tarifa', 'FamilySize', 'Sex' y 'Embarked como entradas; y 'Sobrevivió' como salida.\n\nAdemás, en cuanto a la metodología práctica que comentábamos antes, podemos decir que:\n1. La elección de la métrica de rendimiento es un tema cerrado porque estamos siguiendo las especificaciones de Kaggle.\n2. Nuestro modelo actual puede funcionar como un modelo de referencia y es el resultado de una canalización integral en funcionamiento.\n3. Las curvas de aprendizaje y validación nos permiten monitorear el desempeño del sistema.\n\nComo consecuencia, solo falta el cuarto y último paso de la metodología práctica: mejorar el modelo por iteración. Esto se puede hacer por:\n\n* Mejorar la forma en que manejamos los datos faltantes de 'Edad'. En nuestro enfoque lean, decidimos reemplazar los datos faltantes por un valor único, pero ahora podemos profundizar y buscar una mejor estrategia de imputación.\n* Exploración de datos para comprender qué características pueden tener un impacto en el modelo y cómo se pueden manipular para impulsar ese impacto.\n* Construir nuevas funcionalidades que puedan incrementar el poder predictivo de nuestro modelo.\n\nEsto nos llevará a un proceso pesado de análisis de datos, cuyo objetivo es mejorar el rendimiento del modelo solo por el lado de la calidad de los datos. En otras palabras, no cambiaremos nuestro algoritmo de aprendizaje ni intentaremos mejorar sus parámetros. Solo intentaremos mejorar el rendimiento de nuestro modelo enriqueciendo nuestros datos.\n\nDicho esto, digamos adiós al enfoque magro y demos la bienvenida al enfoque gordito.","metadata":{"_cell_guid":"6e21a2d5-2e4b-4b1e-8ed6-62f8e8c13ad8","_uuid":"42c0913ad47fb9bdbe40a4e1ac6b6ab4734e9840"}},{"cell_type":"code","source":"# Reiniciar conjunto de datos\ndf = df_raw.copy()\ndf.head()","metadata":{"_cell_guid":"5f216dc7-53d3-44a1-ae56-0e396182d4ed","_uuid":"eb6205ad0a55d933dded2878ebe4d6979d84ce8c","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Función de tamaño familiar\ndf['FamilySize'] = df['SibSp'] + df['Parch']\ndf.head()","metadata":{"_cell_guid":"1622ea5f-d72c-4c3f-91a6-2498232460c2","_uuid":"74e5e742a5cdb1039b0bbab58ce2289deace311e","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Soltar SibSp y Parch\ndf.drop('SibSp',axis=1,inplace=True)\ndf.drop('Parch',axis=1,inplace=True)\ndf.head()","metadata":{"_cell_guid":"5b05a66d-1134-4bc1-98e3-4bfd5a7ea447","_uuid":"43a8e8c438c99f32afff12e5345891ee384e5b90","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Eliminar características irrelevantes\ndf.drop(['Name','Ticket','Cabin'], axis=1, inplace=True)\ndf.head()","metadata":{"_cell_guid":"951b794e-35df-42cd-8817-ddbcf84e637e","_uuid":"bde6c8b5d415b80e9c5aa5ec873b894501d39af2","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1. Imputación de datos faltantes de 'Edad'","metadata":{"_cell_guid":"bc1f1eb2-ba6d-4e27-a4cc-efcef69edf48","_uuid":"c7df31e6d86d72347831b53734b6d317f472bdcf"}},{"cell_type":"markdown","source":"Nuestro enfoque inicial para estimar los valores faltantes de 'Edad' fue llenar con un valor de marcador de posición (1000). Esto nos permitió obtener rápidamente un conjunto de datos completo, en el que era fácil identificar los valores imputados. Dado que nuestro objetivo era tener una tubería de extremo a extremo que funcionara lo más rápido posible, este enfoque estuvo bien. Sin embargo, tiene varias limitaciones. Por ejemplo, estamos usando valores de reemplazo poco realistas, que están fuera de rango y distorsionan la distribución de datos. En consecuencia, ahora que estamos mejorando el modelo, tiene sentido desarrollar un método de imputación diferente.\n\nUna forma de mejorar nuestro método de imputación es estimar los valores faltantes en función de las relaciones conocidas. En nuestro caso, podemos hacer esto usando la información en la variable 'Nombre'. Mirando los valores de 'Nombre', podemos ver el nombre y el título de la persona. El título de la persona es una información relevante para estimar edades. Para dar un ejemplo, sabemos que una persona con el título de 'Maestro' es alguien menor de 13 años, ya que ['un niño puede ser llamado maestro solo hasta los 12 años'](http://bit.ly/2HfFHZr) . Por tanto, empleando la información de 'Nombre' podemos mejorar nuestro método de imputación.\n\nLos pasos para implementar este nuevo método de imputación son:\n* Extraer títulos de 'Nombre'.\n* Trace una figura con ambas características y confirme que existe una conexión entre los títulos y la edad.\n* Para cada título, obtenga la edad promedio de las personas y utilícela para completar los valores faltantes.\n\nVeamos cómo funciona esto, antes de que empieces a hundirte en los sentimientos.","metadata":{"_cell_guid":"2c36694c-80c6-48b3-be4b-23576b35942a","_uuid":"1aa117116a8016565a8d62ad217aa20d87ce1e75"}},{"cell_type":"code","source":"# Inspeccionar nombres\ndf_raw['Name'].unique()[:10]","metadata":{"_cell_guid":"cc1c2eac-0f8d-4132-8914-eec28a1797f4","_uuid":"5aacf45feaf5e6830486ce4f5eabbcbc498386c3","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* La regla parece ser: *'apellido'* + *','* + *'título'* + *'otros nombres'*","metadata":{"_cell_guid":"6ba0e20f-1d27-4139-b9eb-2d74956f5b7e","_uuid":"009fbc56d540e3a1f89210511dedf5eca8b9aadf"}},{"cell_type":"code","source":"# Extraer títulos del nombre\ndf['Title']=0\nfor i in df:\n    df['Title']=df_raw['Name'].str.extract('([A-Za-z]+)\\.', expand=False)  # Use REGEX to define a search pattern\ndf.head()","metadata":{"_cell_guid":"548251db-e3d9-4f7e-b2f4-4541d293f093","_uuid":"16c8d75f775cd1666d7c76743e160a1e80240045","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Títulos únicos\ndf['Title'].unique()","metadata":{"_cell_guid":"7956bc9f-83f1-4ee5-9d41-06b5211df02d","_uuid":"510da6e93fe40b26406b203cef5944ab5802a753","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Parcela de barras (títulos, edad y sexo)\nplt.figure(figsize=(15,5))\nsns.barplot(x=df['Title'], y=df_raw['Age']);","metadata":{"_cell_guid":"6d922d6d-814d-4454-8e1d-0c9bca7cf2d8","_uuid":"a1dfe995323f82cc5ad9119b3c30ea8c208a0541","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* El gráfico de barras nos da una estimación de la tendencia central de una variable numérica (altura de cada rectángulo) y una indicación de la incertidumbre en torno a esa estimación (barras de error en negro).\n* Aparte de Rev y Dr, que tienen una barra de error más grande, el valor medio parece representar con precisión los datos de todas las demás funciones. Esto valida nuestro enfoque.\n* [Aquí](https://www.biologyforlife.com/interpreting-error-bars.html) puede encontrar una breve y dulce introducción a la interpretación de las barras de error.","metadata":{"_cell_guid":"40a41f11-2120-4a29-9d90-2e50e6a51b95","_uuid":"52464b8ad28cd2eac259b8b6d86058498f7030fb"}},{"cell_type":"code","source":"# Medios por título\ndf_raw['Title'] = df['Title']  # Para simplificar el manejo de datos\nmeans = df_raw.groupby('Title')['Age'].mean()\nmeans.head()","metadata":{"_cell_guid":"16e5132b-59c8-4732-aed3-e1ba4cec6213","_uuid":"b7dcc55a0f96fb02a515a85274828b616caf3736","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform means into a dictionary for future mapping\nmap_means = means.to_dict()\nmap_means","metadata":{"_cell_guid":"0f3ce996-4369-475b-84bc-058a6afc8b3f","_uuid":"803777860322fd2d0c49338dd5ca63f7fc313eaa","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute ages based on titles\nidx_nan_age = df.loc[np.isnan(df['Age'])].index\ndf.loc[idx_nan_age,'Age'].loc[idx_nan_age] = df['Title'].loc[idx_nan_age].map(map_means)\ndf.head()","metadata":{"_cell_guid":"dc6b5b2b-41f4-4399-944d-5ccc7436298b","_uuid":"ce9d5bc59668e17114a53690c312735807fccdad","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identificar datos imputados\ndf['Imputed'] = 0\ndf.at[idx_nan_age.values, 'Imputed'] = 1\ndf.head()","metadata":{"_cell_guid":"5da28394-6dfc-4c2a-9a2f-b81a36b1bea5","_uuid":"044c47fb3b0702ad77e281fba28032f83aa1f29d","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2. Análisis exploratorio de datos","metadata":{"_cell_guid":"5750e80a-e3da-4382-b097-3e58ee27dba2","_uuid":"257622ae4b7a57c5dcce3099ba03dffad65a8bb7"}},{"cell_type":"markdown","source":"El análisis exploratorio de datos se menciona a menudo como uno de los pasos más importantes en el proceso de análisis de datos. Sin embargo, es bastante fácil caer en una trampa de 'inmersión de datos' (especialmente si está resolviendo problemas sobre barcos hundidos) y perderse en el proceso. Cuando eso sucede, su análisis puede terminar así [así] (https://youtu.be/CIQI3isddbE).\n\nPodemos evitar esto siguiendo un enfoque basado en hipótesis. El enfoque basado en hipótesis consiste en establecer hipótesis sobre el comportamiento de las variables y sus relaciones, al principio del proceso, para luego centrarse en usar datos para probar (o refutar) esas hipótesis. Esto hace que nuestro análisis sea muy objetivo porque recopilaremos los datos suficientes para probar hipótesis específicas. Como resultado, nosotros:\n* Aumentar la velocidad. Ya que limitaremos nuestro análisis a alguna hipótesis y seguiremos adelante.\n* Reducir el esfuerzo. La cantidad de datos y el número de pruebas será solo lo necesario para verificar su hipótesis.\n* Reducir el riesgo. Si aciertas, triunfas rápido, si te equivocas, fracasas rápido.\n\n[Aquí](http://gsl.mit.edu/media/programs/ghana-summer-2013/materials/problem_solution_grand_slam_7_steps_to_master_training_deck.pdf) puede encontrar una de mis presentaciones de PowerPoint favoritas sobre los beneficios y procedimientos de un enfoque basado en hipótesis en resolución de problemas Tenga en cuenta que, particularmente cuando realmente necesita aprender sobre el conjunto de datos, tiene sentido colocar el cilindro de buceo y sumergirse en las profundidades del análisis de datos. Sin embargo, si al principio puede generar una suposición fundamentada de cuál es la respuesta de su problema, creo que debe probar su hipótesis y aprender de ella lo más rápido que pueda.\n\nBien, ahora que te convencí de que el enfoque basado en hipótesis es la [última coca cola en el desierto] (http://bit.ly/2oYzJ7U), déjame mostrarte cómo aplicarlo. Casos como el que tenemos son objetivos fáciles para el enfoque basado en hipótesis porque no tenemos muchas variables, por lo que podemos adivinar más o menos su impacto. En consecuencia, comenzaremos enumerando cada una de las variables y generando hipótesis sobre su relación con la variable objetivo ('Sobrevivió'). Luego, probaremos esas hipótesis a través de un conjunto de herramientas de análisis de datos exploratorios. Como resultado, obtendremos una visión completa de las variables que deberían pertenecer a nuestro modelo de predicción.\n\nEmpecemos:\n\n* **Id del Pasajero**. Esta es solo una identificación única de cada pasajero. No se espera que sea relevante para nuestro análisis.\n* **Sobrevivió**. Variable objetivo. Hundirse o no hundirse es la cuestión de este ejercicio.\n* **Clase P**. Esta es la clase de boleto. Según Karl Marx, esto debería afectar nuestra variable objetivo. La primera clase debería tener una tasa de supervivencia más alta.\n* **Nombre**. Los nombres son una forma de etiquetado social, especialmente cuando van acompañados de un título. Como consecuencia, puede dar lugar a diferentes formas de tratamiento. Mantengamos un ojo en esto.\n* **Sexo**. Siempre importante.\n* **Edad**. Debería marcar la diferencia. Por ejemplo, los niños suelen ser evacuados primero en un desastre, para que podamos pensar en una solución en silencio... Bromeando, la verdadera razón por la que la 'Edad' importa es esta [una] (http://www.dailymail.co) .uk/sciencetech/article-1254788/Por qué-mujeres-niños-salvados-Titanic-Lusitania.html#ixzz54KETWEPr).\n* **Esp.Sib**. Número de hermanos/cónyuges a bordo del Titanic. Diría que es más fácil sobrevivir si estás con tu familia que si viajas solo. [Trabajo en equipo](https://youtu.be/1qzzYrCTKuk)!\n* **Parque**. Número de padres/hijos a bordo del Titanic. Debería jugar con 'SibSp'.\n* **Boleto**. Este es el número de billete. A menos que tenga alguna información sobre lugares, no debería ser importante para propósitos de predicción.\n* **Tarifa**. La misma lógica que 'Pclass'.\n* **Cabina**. El número de cabina puede indicar dónde estaban las personas durante el desastre. No sería sorprendente que tuviera alguna influencia en las posibilidades de supervivencia, pero esta variable se excluyó debido al alto porcentaje de valores faltantes.\n* **Embarcado**. Cuando sale el sol, sale para todos. No es de esperar que la gente que viene de Cherburgo tenga más mala suerte que la gente que viene de Southampton. A menos que haya algún efecto de segundo orden, [como negarse a huir para mantener su honor como hombre] (http://www.mindblowing-facts.org/2013/07/the-only-japanese-who-survived-the -titanic-perdió-su-trabajo-por-que-era-conocido-como-cobarde-en-japon-por-no-morir-con-los-otros-pasajeros/), yo diría que esta variable es no importante.\n\nAhora, paso a paso, realicemos nuestro análisis.","metadata":{"_cell_guid":"3578d65f-0c7a-4bc3-a7fa-e788e463e57f","_uuid":"43547eb4335ee508e030bddcab7ab10d3d96531d"}},{"cell_type":"markdown","source":"### 2.2.1. Pclass","metadata":{"_cell_guid":"77e1f23f-8a83-4be1-ab7b-314c8195d650","_uuid":"29ab291a611e5f751d61fedeb4b09d88de342efc"}},{"cell_type":"markdown","source":"Nuestra hipótesis es que cuanto mayor sea la clase, mayores serán las posibilidades de supervivencia. Esto significa que una persona que viaja en primera clase tiene más posibilidades de sobrevivir que una persona que viaja en segunda o tercera clase.\n\nPara visualizar si existe una relación entre 'Pclass' y 'Survival', hagamos un gráfico de barras.","metadata":{"_cell_guid":"36df07bb-179f-439a-9ccc-9203e91c2002","_uuid":"6625171e3fcba6d055d028a455aee1741e5edfe6"}},{"cell_type":"code","source":"# Plot\nsns.barplot(df['Pclass'],df['Survived']);","metadata":{"_cell_guid":"c0fc1b34-7d55-486f-b6e6-458702fb33f4","_uuid":"6a08ee4ded9d87a7eeef702f491846b7c1c649d5","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nuestra hipótesis es que cuanto mayor sea la clase, mayores serán las posibilidades de supervivencia. Esto significa que una persona que viaja en primera clase tiene más posibilidades de sobrevivir que una persona que viaja en segunda o tercera clase.\n\nPara visualizar si existe una relación entre 'Pclass' y 'Survival', hagamos un gráfico de barras.","metadata":{"_cell_guid":"1cbe0c7a-966e-40b5-8419-57d2d87f51cf","_uuid":"ac21502d7afb4523802117077be271c33a3ac259"}},{"cell_type":"markdown","source":"### 2.2.2. Name/Title","metadata":{"_cell_guid":"4d2a67a0-6c27-40fe-89f5-027831f9e380","_uuid":"4cc2638cb77faab3fadb4065b3f28e3ad94805e8"}},{"cell_type":"markdown","source":"Nuestra suposición es que el título de las personas influye en cómo son tratadas. En nuestro caso, tenemos varios títulos, pero solo algunos de ellos son compartidos por un número importante de personas. Por ello, sería interesante que pudiéramos agrupar algunos de los títulos y simplificar nuestro análisis.\n\nAnalicemos el título y veamos si podemos encontrar una manera sensata de agruparlos. Luego, probamos nuestros nuevos grupos y, si funciona de manera aceptable, lo mantenemos. Por ahora, la optimización no será un objetivo. La atención se centra en conseguir algo que pueda mejorar nuestra situación actual.","metadata":{"_cell_guid":"b99ac5bc-8e76-423c-9548-64849e2d6ccb","_uuid":"9f5fa19edf62b46ed4a274cf301754187cd09536"}},{"cell_type":"code","source":"# Cuenta cuantas personas tienen cada uno de los títulos\ndf.groupby(['Title'])['PassengerId'].count()","metadata":{"_cell_guid":"32db00e7-8bfa-4e97-9f68-d8857df8b577","_uuid":"b83281cab9a1a6d75c8fd762208ba390db86f53b","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"De los resultados anteriores podemos ver que:\n\n* Títulos como 'Maestro', 'Señorita', 'Señor' y 'Señora' aparecen varias veces. En consecuencia, no los agruparemos.\n* Con respecto a Mme y Mlle, podemos ver [aquí](https://www.frenchtoday.com/blog/french-culture/madame-or-mademoiselle-a-delicate-question) que corresponden a las categorías Señora y Señorita , respectivamente. En consecuencia, los asignaremos a esos títulos.\n* Finalmente, agruparemos todos los demás títulos en un nuevo título llamado 'Otro'. Luego, definiremos 'Título' como una característica categórica y la trazaremos para ver cómo se ve. Si se ve bien, procederemos con esta nueva categorización.","metadata":{"_cell_guid":"c6f63af9-a582-4b60-bbe1-d0fa3fe184dd","_uuid":"1672ae4ebf876cb8904808603c551aa404c900a6"}},{"cell_type":"code","source":"# Mapa de títulos agregados:\ntitles_dict = {'Capt': 'Other',\n               'Major': 'Other',\n               'Jonkheer': 'Other',\n               'Don': 'Other',\n               'Sir': 'Other',\n               'Dr': 'Other',\n               'Rev': 'Other',\n               'Countess': 'Other',\n               'Dona': 'Other',\n               'Mme': 'Mrs',\n               'Mlle': 'Miss',\n               'Ms': 'Miss',\n               'Mr': 'Mr',\n               'Mrs': 'Mrs',\n               'Miss': 'Miss',\n               'Master': 'Master',\n               'Lady': 'Other'}","metadata":{"_cell_guid":"3269ceef-6139-4067-a876-518990c01d42","_uuid":"c14e88a44d85b73a02aacf946f8b1a143c36eefc","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# títulos de grupo\ndf['Title'] = df['Title'].map(titles_dict)\ndf['Title'].head()","metadata":{"_cell_guid":"3512b140-c3b3-4056-bd4d-7923ce6a68dc","_uuid":"be1f32a067017ee70f0c98e53932d303327a102a","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transformar en categórico\ndf['Title'] = pd.Categorical(df['Title'])\ndf.dtypes","metadata":{"_cell_guid":"7a9f787c-ba4f-45be-9e9e-50db2e7a5897","_uuid":"83012b55fdc060ea08c1371051d0e747bd5e2525","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot\nsns.barplot(x='Title', y='Survived', data=df);","metadata":{"_cell_guid":"616dcff2-71f1-4bb4-8fb1-24aa087ce3b8","_uuid":"393b112f48ee465800e8f6c8d7319f5ee7c80c61","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como ya sabemos, el diagrama de barras nos muestra una estimación del valor medio (altura de cada rectángulo) y una indicación de la incertidumbre alrededor de esa tendencia central (barras de error).\n\nNuestros resultados sugieren que:\n* Las personas con el título 'Señor' sobrevivieron menos que las personas con cualquier otro título.\n* Los títulos con una tasa de supervivencia superior al 50% son los que corresponden a títulos femeninos (Miss o Mrs) o infantiles (Master).\n* Nuestra nueva categoría, 'Otros', debería ser más discreta. Como podemos ver en la barra de error (línea negra), existe una incertidumbre significativa en torno al valor medio. Probablemente, uno de los problemas es que estamos mezclando títulos masculinos y femeninos en la categoría 'Otros'. Deberíamos proceder con un análisis más detallado para resolver esto. Además, la categoría 'Maestro' parece tener un problema similar. Por ahora, no haremos ningún cambio, pero mantendremos estas dos situaciones en mente para futuras mejoras de nuestro conjunto de datos.","metadata":{"_cell_guid":"04aae632-7fbd-4576-a0ef-38216081f755","_uuid":"f3a6a1909ad7674a658db651b0b2c09e17ff6c61"}},{"cell_type":"markdown","source":"### 2.2.3. Sex","metadata":{"_cell_guid":"625d1104-cb15-496a-9523-db90953ef3ca","_uuid":"d39d9265f11fd62e0bb9bea1f622e8058b20458c"}},{"cell_type":"markdown","source":"El sexo es uno de los temas más discutidos en la historia de la humanidad. Hay varias perspectivas sobre el tema, pero debo confesar que las perspectivas de Freud tuvieron un impacto significativo en mí porque me han mostrado el tema en una nueva perspectiva. Lo nuevo de Freud es que asoció muchos comportamientos \"normales\" a los impulsos sexuales, casi hasta el punto de hacernos cuestionar todo lo que hacemos. Al final, Freud se dio cuenta de que no todo se trataba de sexo. Como dijo, 'a veces un cigarro es solo un cigarro' (Freud solía fumar cigarros).\n\nDejando los preámbulos a un lado, lo que realmente necesitamos saber es si a veces un cigarro es solo un cigarro o no. Ya tenemos algunas pistas de que, en Titanic, las mujeres tenían una mayor tasa de supervivencia. Pero, nada mejor que una trama para ver qué está pasando.","metadata":{"_cell_guid":"087d5dd6-96ca-4fa6-bd95-b7ce40019543","_uuid":"e7db8896bf50d1c307555eeb6970b60997df0b7e"}},{"cell_type":"code","source":"# Transformar en categórico\ndf['Sex'] = pd.Categorical(df['Sex'])","metadata":{"_cell_guid":"c2a27fc2-f5b0-486c-90e9-64a93fd1f3df","_uuid":"ab9a73dc86033a709449ab4c02a309f822c641f2","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot\nsns.barplot(df['Sex'],df['Survived']);","metadata":{"_cell_guid":"94c68aa1-d4af-4d52-9806-846c69c35e95","_uuid":"07bfbb7714d8b69e344a959bdf638717e8b84fb4","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nuestra hipótesis parece correcta. El mundo pertenece a las mujeres, y el Titanic también.","metadata":{"_cell_guid":"3c76cfbe-dddf-4295-bbe6-9b4936bbc555","_uuid":"12b6efd42b011b8c6ef3447dbea7dd6981c70a3a"}},{"cell_type":"markdown","source":"### 2.2.4. Age","metadata":{"_cell_guid":"0c4ad2b1-c3bc-4e17-9d3d-dbd41b511bc9","_uuid":"670abc340c5b39e33f7678d79c9cee991dd4ae35"}},{"cell_type":"markdown","source":"'Edad' es la siguiente variable en la lista. Nuestra hipótesis es que los niños son más propensos a sobrevivir, mientras que las personas en su vida adulta pueden tener una menor tasa de supervivencia. Personalmente, no tengo ninguna intuición especial sobre los ancianos, ya que son los más vulnerables. Esto puede jugar para ambos lados: o las personas ayudan a los ancianos porque son más vulnerables, o no pueden hacer frente a los desafíos que plantea el naufragio de un barco.\n\nLlamemos al sospechoso habitual (gráfico de barras) para que nos ayude a comprender la situación.","metadata":{"_cell_guid":"96b3ab57-51c2-4617-9964-08d6a71f9ed3","_uuid":"9398e7c00d9ca9f302e1b57b702ac9c13f7f2e5f"}},{"cell_type":"code","source":"# Plot\nplt.figure(figsize=(25,10))\nsns.barplot(df['Age'],df['Survived'], ci=None)\nplt.xticks(rotation=90);","metadata":{"_cell_guid":"9feef137-8257-49e4-92f6-7e42e91dbf7d","_uuid":"afd193d3a3b865f45df21310dbec50b894b42e2e","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Con un poco de creatividad, podemos decir que la trama tiene tres regiones:\n\n1. Una región que va entre los 0 y los 15 años;\n2. Uno entre 15 y 48 años;\n3. Una última entre los 48 y los 80 años.\n\nSé que esta división es discutible, especialmente en lo que concierne a las dos últimas categorías. Sin embargo, el punto es que esta división de categorías se ajusta a lo que sabemos sobre la forma en que se organiza nuestra sociedad: niños, adultos y ancianos. Por ahora, procedamos de esta manera.","metadata":{"_cell_guid":"ea258a36-30c9-4f6a-b797-6bbf995f2668","_uuid":"d0cc09e0d31bca2edcf49299dc2348f02202cbb7"}},{"cell_type":"code","source":"# Plot\n'''\nProbablemente, hay una manera más fácil de hacer esta trama. Tuve un problema al usar\nplt.axvspan porque los valores xmin y xmax no eran\nsiendo graficado correctamente. Por ejemplo, definiría xmax = 12 y solo\nel área entre 0 y 7 estaría llena. Esto sucedía porque mi\nEl eje X no sigue una secuencia regular (0, 1, ..., n). Después de algún juicio\ny error, noté que xmin y xmax se refieren a la cantidad de elementos en\nla coordenada del eje X que debe rellenarse. En consecuencia, definí dos\nvariables, x_limit_1 y x_limit_2, que cuentan el número de elementos que\ndebe completarse en cada intervalo. ¿Suena confuso? Para mi también.\n'''\nlimit_1 = 12\nlimit_2 = 50\n\nx_limit_1 = np.size(df[df['Age'] < limit_1]['Age'].unique())\nx_limit_2 = np.size(df[df['Age'] < limit_2]['Age'].unique())\n\nplt.figure(figsize=(25,10))\nsns.barplot(df['Age'],df['Survived'], ci=None)\n\nplt.axvspan(-1, x_limit_1, alpha=0.25, color='green')\nplt.axvspan(x_limit_1, x_limit_2, alpha=0.25, color='red')\nplt.axvspan(x_limit_2, 100, alpha=0.25, color='yellow')\n\nplt.xticks(rotation=90);","metadata":{"_cell_guid":"4d505047-b82b-4d06-b680-8f62b3e307b7","_uuid":"7f3bb9e09c00899427997213426e445b9fc2a125","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bin data\ndf['Age'] = pd.cut(df['Age'], bins=[0, 12, 50, 200], labels=['Child','Adult','Elder'])\ndf['Age'].head()","metadata":{"_cell_guid":"f056b07e-5c0d-4252-8fc5-69e72b3ce70b","_uuid":"8290015f8eb5b5dc1ad99a906be55dd3361b8f77","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot\nsns.barplot(df['Age'], df['Survived']);","metadata":{"_cell_guid":"88c3ca8f-8374-469d-9700-ae537f11f01d","_uuid":"65ed4dd726cb3ef82024d73f7a049fd289455737","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La gráfica muestra que los niños tienen una mayor tasa de supervivencia. También muestra que, en términos de supervivencia, no hay una diferencia significativa entre las categorías 'Adulto' y 'Anciano'. Por ahora, no haremos ningún cambio porque hay una justificación teórica detrás de esta categorización. Sin embargo, parece que bastaría con distinguir entre niños y adultos.","metadata":{"_cell_guid":"1978ab71-a551-48ec-99f4-481ad8d29cfc","_uuid":"969f45f01389ea696146ca0cbbd4d7e9922930e5"}},{"cell_type":"markdown","source":"### 2.2.5. FamilySize","metadata":{"_cell_guid":"eb4ded78-f48e-4155-9fe2-9778f66a05a2","_uuid":"f4d5198a277f4609e8143328f1baa46e1f80fe70"}},{"cell_type":"markdown","source":"En cuanto al tamaño de la familia, nuestra hipótesis es que aquellos que viajan solos, tienen una menor tasa de supervivencia. La idea es que las personas en familia puedan colaborar y ayudarse a escapar.\n\nVeamos si eso tiene sentido usando nuestro [hermoso y único amigo] (https://youtu.be/LsQtnBu3p7Y), el gráfico de barras.","metadata":{"_cell_guid":"ec7f5aa5-1d81-4047-b934-b7abf570c3a6","_uuid":"349129f045e588758e844cad810f39d76413838f"}},{"cell_type":"code","source":"# Plot\nsns.barplot(df['FamilySize'], df['Survived']);","metadata":{"_cell_guid":"f48b71af-0c3d-4da0-9ad2-aecdaf2e3c1d","_uuid":"e7391a4547c02a0a5ccde9262fbfa36ff086cf56","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como podemos ver, cuando 'FamilySize' está entre 0 y 3, nuestra hipótesis encuentra algo de apoyo. Las personas que viajan solas tienen una tasa de supervivencia más baja que las personas que viajan con una, dos o tres personas más.\n\nSin embargo, cuando FamilySize está entre 4 y 10, las cosas comienzan a cambiar. A pesar de la gran variabilidad de los resultados, la tasa de supervivencia desciende. Esto puede sugerir que nuestra hipótesis debe revisarse cuando 'FamilySize' es mayor que 3.\n\nEsta variable parece ser más compleja de lo esperado. Por tanto, no haremos ninguna transformación en esta variable y la dejaremos como variable continua para conservar toda la información que tiene.","metadata":{"_cell_guid":"dc9f94fb-2cdc-4a7a-912b-778ba047f574","_uuid":"59bc0b145672167461e2fe4f34dae840ae9eafb2"}},{"cell_type":"markdown","source":"### 2.2.6. Fare","metadata":{"_cell_guid":"060f7118-17a2-4a31-9295-34332529ada0","_uuid":"3468173b1accdddb98d5300c71e0a97437c61bd9"}},{"cell_type":"markdown","source":"La misma lógica aplicada a 'Pclass' debería funcionar para 'Tarifa': tarifas más altas, mayor tasa de supervivencia.\n\nDado que ahora queremos establecer comparaciones entre diferentes niveles de una variable categórica, usaremos un gráfico de caja en lugar de un gráfico de barras.","metadata":{"_cell_guid":"5f0336d5-8ed9-4ba5-b8ea-22975aaca19a","_uuid":"4fbfd7b50aca244528b9285c4c7add374f925525"}},{"cell_type":"code","source":"# Plot\nplt.figure(figsize=(7.5,5))\nsns.boxplot(df['Survived'], df['Fare']);","metadata":{"_cell_guid":"09e1f13c-b3fb-499d-a941-9e46fffa26fc","_uuid":"1446a7dd99fe3e7ba00d27e5fbceec5e1c2be602","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La trama sugiere que los que sobrevivieron pagaron una tarifa más alta. Como creemos que esta variable está conectada con 'Pclass', veamos cómo funcionan juntas.","metadata":{"_cell_guid":"c3536758-0482-4d0e-8054-0c31caaeb650","_uuid":"fb35593f956aaec48156e1cc0c6aeb1950adaa4f"}},{"cell_type":"code","source":"# Plot\nsns.barplot(df['Survived'], df['Fare'], df['Pclass']);","metadata":{"_cell_guid":"18aef4b4-86e4-4dde-b565-c1958ed28d3e","_uuid":"bf8346f9fb228927a41c219e36c366dd9c026fff","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Aquí tenemos un resultado interesante. Parece que la 'Tarifa' no marca la diferencia, en términos de supervivencia, si viaja en segunda o tercera clase. Sin embargo, si viaja en primera clase, cuanto mayor sea la tarifa, mayores serán las posibilidades de supervivencia. Teniendo esto en cuenta, tendría sentido crear funciones de interacción entre 'Tarifa' y 'Pclass'.","metadata":{"_cell_guid":"0a0fe60c-9092-4c6e-8441-8698c5da48d6","_uuid":"01ebc56582a8698858bf5c5ef243413f07721171"}},{"cell_type":"markdown","source":"### 2.2.7. embarcado","metadata":{"_cell_guid":"ebff0f2a-283d-4b77-938c-d5f4a6b3e14d","_uuid":"19ef6fc7013e030fb4de1c809d5c3b814457f9c5"}},{"cell_type":"markdown","source":"La hipótesis sobre 'Embarcado' es que no influye en las posibilidades de supervivencia. Es difícil imaginar un escenario en el que la gente de Southampton, por ejemplo, tuviera una ventaja competitiva tal que los hiciera más aptos para sobrevivir que la gente de Queensland. Sí, en [Darwin](https://en.wikipedia.org/wiki/Natural_selection) creemos.\n\nUna simple trama puede iluminarnos.","metadata":{"_cell_guid":"68a4e07b-5fc0-4d00-89d4-4a2ff21f2f4e","_uuid":"e68cf9b8780bf85491d60d10439e82533a6280de"}},{"cell_type":"code","source":"# Plot\nsns.barplot(df['Embarked'], df['Survived']);","metadata":{"_cell_guid":"755156f5-0d24-445b-a3ff-24bcce90c00b","_uuid":"03523e904b2bea5c90e008368b8c754a20597089","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ups... Parece que las personas que se embarcan en C fueron seleccionadas por una entidad superior para sobrevivir. Esto es extraño y puede estar ocultando alguna relación que no es obvia con esta trama (por ejemplo, las personas que se embarcaron en C eran en su mayoría mujeres).\n\nProfundicemos más.","metadata":{"_cell_guid":"9771a658-9339-4cee-95d4-2c04780ba602","_uuid":"8c62a536cbb6a7d64dcdc51089a6f9c1666bac3b"}},{"cell_type":"code","source":"# Comparar con otras variables\ndf.groupby(['Embarked']).mean()","metadata":{"_cell_guid":"f43582d1-8ab3-4b42-8764-ce602456ce2f","_uuid":"55bbd0304881dd12cbb52d29a4218180f247933c","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Parece que las personas que se embarcan en C pagaban más y viajaban en mejor clase que las personas que se embarcaban en Q y S.","metadata":{"_cell_guid":"e52b4157-b99d-4b8b-9740-680d7a6c5d21","_uuid":"27fc68b077f7cdda4fdd1f8d4cc7f29950fbdd38"}},{"cell_type":"code","source":"# Relación con la edad\ndf.groupby(['Embarked','Age'])['PassengerId'].count()","metadata":{"_cell_guid":"09a5ff2f-08e3-4d9f-92d3-7ba3efb4322c","_uuid":"63f51d0c2a9846fbfa81e1371077a0d687a77987"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No significant differences can be found.","metadata":{"_cell_guid":"a2e5cffe-de90-49d4-aa60-c66801ebb6be","_uuid":"81d8eac557a3aa273c016a23b5d794bd9b6a97d2"}},{"cell_type":"code","source":"# Relación con el sexo\ndf.groupby(['Embarked','Sex'])['PassengerId'].count()","metadata":{"_cell_guid":"db6bfd59-16d1-4102-bafb-d0c52bc8505e","_uuid":"3d7f1fb46f04dcabc53f3fe89bb0af5e90260b7e","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No se pueden encontrar diferencias significativas.","metadata":{"_cell_guid":"2c0cae6c-758e-4f7f-b90a-22dd37e79b42","_uuid":"6cfa14a2364db1a93d8eeddc575a235ba4c66b39"}},{"cell_type":"markdown","source":"Teniendo en cuenta los resultados anteriores, me siento tentado a decir que el punto de embarque no influye en la tasa de supervivencia. Lo que realmente parece estar influyendo es la clase en la que viajaba la gente y cuánto gastaba.\n\nPor ahora, no eliminaré la variable porque siento que soy un poco parcial y trato de forzar una conclusión. Sin embargo, tengamos en cuenta que tal vez 'Embarcado' no afecte a 'Sobrevivido'.","metadata":{"_cell_guid":"0c8eed9c-6788-4020-8760-38d28d0b38b7","_uuid":"c1f87ad7f1a9c93287fa5702590efc48e671e913"}},{"cell_type":"markdown","source":"## 2.3. Extracción de características","metadata":{"_cell_guid":"16c2313a-3eec-4583-92f8-bef922adc34b","_uuid":"56c63a287552a294fa26efba6061215e468c5858"}},{"cell_type":"markdown","source":"En el libro ['How Google Works'](https://amzn.to/2M9hvv7), Eric Schmidt y Jonathan Rosenberg refieren que el ingrediente secreto de Google es la 'percepción técnica'. Según los autores, es el conocimiento técnico fundamental que permite a las empresas crear excelentes productos, que brindan un valor real a los clientes. Por ejemplo, [PageRank](http://www.cs.princeton.edu/~chazelle/courses/BIB/pagerank.htm) dio una ventaja competitiva increíble a Google en relación con otros motores de búsqueda, al proporcionar una forma mucho mejor para clasificar los resultados de búsqueda en la web.\n​\nLa extracción de características es nuestra visión tecnológica en el aprendizaje automático. Aborda el problema de lograr el conjunto de funciones más informativo y compacto para mejorar el rendimiento de los modelos de aprendizaje automático. Vayamos paso a paso. En primer lugar, estamos hablando de 'informativo'. Esto significa que estamos buscando características que puedan caracterizar el comportamiento de lo que estamos tratando de modelar. Por ejemplo, si queremos modelar el clima, características como la temperatura, la humedad y el viento son informativas (están relacionadas con el problema). Por el contrario, el resultado de un partido de fútbol no será una característica informativa porque no afecta el clima.\n​\nCon respecto a 'compacto', lo que queremos decir es que queremos excluir características irrelevantes de nuestro modelo. Hay varias razones para excluir características irrelevantes. En nuestro caso, diría que lo más importante es reducir el sobreajuste. Volviendo al ejemplo del clima: sabemos que los puntajes de fútbol no afectan el clima, pero supongamos que todos los casos de lluvia en nuestro conjunto de entrenamiento ocurren después de una victoria del [Benfica](https://youtu.be/qX0D7xkF33s). Entonces, nuestro modelo podría aprender que la lluvia está relacionada con las victorias del Benfica, lo cual no es cierto. Tal generalización incorrecta a partir de una característica irrelevante del conjunto de entrenamiento daría como resultado un modelo de aprendizaje automático que se ajusta a un conjunto particular de datos, pero no puede predecir las observaciones futuras de manera confiable (sobreajuste).\n​\nEstas dos cuestiones principales se abordan en las siguientes subsecciones:\n1. Ingeniería de características, que está relacionada con la generación de características informativas;\n2. Selección de funciones, que se refiere a la elección de un conjunto compacto de funciones.","metadata":{"_cell_guid":"093d98a3-0e28-447f-8f2f-add482629766","_uuid":"df5b074c2a1e9c502e54ea2527ca74743a9fc7c9"}},{"cell_type":"markdown","source":"### 2.3.1. Ingeniería de características","metadata":{"_cell_guid":"e36d5980-601d-41d1-bba8-7198981aeefc","_uuid":"409c1d3ab404db634aefe8bda35244ea63be29e4"}},{"cell_type":"markdown","source":"La ingeniería de características es el arte de convertir datos sin procesar en características útiles. Hay varias técnicas de ingeniería de funciones que puede aplicar para ser un artista. [Heaton (2016)](https://arxiv.org/pdf/1701.07852.pdf) presenta una lista completa de ellos. Usaremos solo dos técnicas:\n* Transformaciones de Box-Cox [(Box & Cox 1964)](https://www.nuffield.ox.ac.uk/users/cox/cox72.pdf);\n* Generación de polinomios mediante desarrollos no lineales.\n\nAntes de la aplicación de estas técnicas, solo haremos algunos ajustes a los datos para prepararlos para el proceso de modelado.","metadata":{"_cell_guid":"52c60034-20da-404d-8ef5-6f1689b5222f","_uuid":"3ef1d8f74e939815b6203162aaef0b05dd8f2041"}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"#### Preparación de datos","metadata":{"_cell_guid":"9ce8a0ad-5499-4d86-8220-f3151ae0a592","_uuid":"971426b7c688eb8034ea1e171a00b1b8153afff4"}},{"cell_type":"code","source":"# Overview\ndf.head()","metadata":{"_cell_guid":"233a622d-2f95-4233-abfa-f559d4c22bf0","_uuid":"03810a5fd9ec6527856fdd983f7dcc4bbe8c88c3","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop feature\ndf.drop('PassengerId', axis=1, inplace=True)","metadata":{"_cell_guid":"976964d6-8d83-4c31-a9b2-82e0b5d403ca","_uuid":"b32802845c149a64462f0a1a02e96a89f4e247d1","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check features type\ndf.dtypes","metadata":{"_cell_guid":"60af4c71-c20a-4a1c-a028-5b738eab617b","_uuid":"22dc3d3904d48970974191c9d332773e3266d215","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform object into categorical\ndf['Embarked'] = pd.Categorical(df['Embarked'])\ndf['Pclass'] = pd.Categorical(df['Pclass'])\ndf.dtypes","metadata":{"_cell_guid":"9d9001bd-7311-44ab-8474-a545ef47049b","_uuid":"92e0c5844b5615d5903ca7286607bee72b42e68c","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform categorical features into dummy variables\ndf = pd.get_dummies(df, drop_first=1)  \ndf.head()","metadata":{"_cell_guid":"d53a5c89-5043-401c-948c-a9e4be7caa7a","_uuid":"2c332a0e0a0c779a5204f52aef08c0eb65cd8141","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get training and test sets\nfrom sklearn.model_selection import train_test_split\n\nX = df[df.loc[:, df.columns != 'Survived'].columns]\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)","metadata":{"_cell_guid":"58c13871-0b97-477b-92e1-43bde05dc27c","_uuid":"ad3884b307f0d45ef332a4da46d32bf1589154b4","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Transformaciones de Box-Cox","metadata":{"_cell_guid":"4b55aad6-b65f-4349-a73f-72302ef55e07","_uuid":"d1b53a233fb4f0d7aede88e612c9bb3efe3bffdb"}},{"cell_type":"markdown","source":"Las transformaciones de Box-Cox tienen como objetivo normalizar las variables. Estas transformaciones son una alternativa a las transformaciones típicas, como las transformaciones de raíz cuadrada, las transformaciones logarítmicas y las transformaciones inversas. La principal ventaja de las transformaciones de Box-Cox es que normalizan de manera óptima la variable elegida. Por lo tanto, evitan la necesidad de probar aleatoriamente diferentes transformaciones y automatizan el proceso de transformación de datos.","metadata":{"_cell_guid":"b29a1f6f-f863-4314-ba25-0748ccc292de","_uuid":"dc50b06d26575c3c363cbcbfc3f8dda95807f054"}},{"cell_type":"code","source":"# Aplicar la transformación de Box-Cox\nfrom scipy.stats import boxcox\n\nX_train_transformed = X_train.copy()\nX_train_transformed['Fare'] = boxcox(X_train_transformed['Fare'] + 1)[0]\nX_test_transformed = X_test.copy()\nX_test_transformed['Fare'] = boxcox(X_test_transformed['Fare'] + 1)[0]","metadata":{"_cell_guid":"062912ef-bd0f-496f-9d13-d2a883e3375c","_uuid":"fbc6351ab708e981d048abdda80e17b6b1bfd252","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Polynomials","metadata":{"_cell_guid":"8fbc93dc-ea8e-435c-a124-9522c6be498f","_uuid":"307a99182a9b4a9af3cf117665b1c7d9aebcbe8d"}},{"cell_type":"markdown","source":"Una forma estándar de enriquecer nuestro conjunto de características es generar polinomios. La expansión polinomial crea interacciones entre características, así como crea potencias (por ejemplo, el cuadrado de una característica). De esta forma introducimos una dimensión no lineal a nuestro conjunto de datos, lo que puede mejorar el poder predictivo de nuestro modelo.\n\nDebemos escalar nuestras características cuando tenemos polinomios o términos de interacción en nuestro modelo. Estos términos tienden a producir [multicolinealidad](https://en.wikipedia.org/wiki/Multicollinearity), lo que puede hacer que nuestras estimaciones sean muy sensibles a cambios menores en el modelo. Escalar características a un rango nos permite reducir la multicolinealidad y sus problemas.\n\nPara escalar las características, transformaremos los datos para que se encuentren entre un valor mínimo y máximo dado. Seguiremos la práctica común y diremos que nuestro valor mínimo es cero y nuestro valor máximo es uno.","metadata":{"_cell_guid":"ef8f485d-fe3f-4753-bc6f-008a4bad8ffd","_uuid":"74701e7b0fddc9f955f0f160e9df7191be875721"}},{"cell_type":"code","source":"# Reescalar datos\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX_train_transformed_scaled = scaler.fit_transform(X_train_transformed)\nX_test_transformed_scaled = scaler.transform(X_test_transformed)","metadata":{"_cell_guid":"8798cdd4-423b-41ae-84eb-cd6a9d6d175b","_uuid":"2988268bb20715ac9c60efa896b5da21ba858b67","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtener características polinómicas\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2).fit(X_train_transformed)\nX_train_poly = poly.transform(X_train_transformed_scaled)\nX_test_poly = poly.transform(X_test_transformed_scaled)","metadata":{"_cell_guid":"2726c248-29c1-4b52-a406-034278016cc6","_uuid":"723bb510ae8075c9138867cd0bc68b12b8eefe5b","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Debug\nprint(poly.get_feature_names())","metadata":{"_cell_guid":"799cf33a-ab05-44dc-b717-90b9d2dbd2a4","_uuid":"3fdd9c97dc3dbd9fd2f5c903a9a53edf68198d5f","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3.2. Selección de características","metadata":{"_cell_guid":"ee8f4a27-42f1-418e-9a84-ff2882c0e8e1","_uuid":"90cc0fe283b0c6226bcfa30615dc00b923bfe6c3"}},{"cell_type":"markdown","source":"El siguiente paso es realizar la selección de características. La selección de características se trata de elegir la información relevante. Es bueno agregar y generar funciones, pero en algún momento debemos excluir funciones irrelevantes. De lo contrario, estaremos penalizando el poder predictivo de nuestro modelo. Puede encontrar una introducción concisa al tema de la selección de funciones en [Guyon & Elisseeff (2003)] (http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf).\n\nEn este trabajo, utilizaremos un enfoque estadístico univariante. Este enfoque selecciona características en función de pruebas estadísticas univariadas entre cada característica y la variable de destino. La intuición es que las características que son independientes de la variable objetivo son irrelevantes para la clasificación.\n\nUsaremos la prueba de chi-cuadrado para la selección de características. Esto significa que tenemos que elegir el número de características que queremos en el modelo. Por ejemplo, si queremos tener tres características en nuestro modelo, el método seleccionará las tres características con la puntuación $\\chi^2$ más alta.\n\nDado que no conocemos el número ideal de funciones, probaremos el método con todas las funciones posibles y elegiremos la cantidad de funciones con mejor rendimiento.","metadata":{"_cell_guid":"29c7f79f-67a9-4128-9974-14d2e32b6625","_uuid":"0b8fd7942e23c275247494498e2692535e0c555b"}},{"cell_type":"markdown","source":"#### Univariate statistics","metadata":{"_cell_guid":"3c385be1-82b0-4855-995b-62945d002963","_uuid":"3c5362e1267dc6f27db0ffa0c26a7826ec1c7260"}},{"cell_type":"code","source":"# Seleccione características usando la prueba de chi-cuadrado\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n## Obtener puntaje usando el modelo original\nlogreg = LogisticRegression(C=1)\nlogreg.fit(X_train, y_train)\nscores = cross_val_score(logreg, X_train, y_train, cv=10)\nprint('CV accuracy (original): %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\nhighest_score = np.mean(scores)\n\n## Obtenga puntaje usando modelos con selección de características\nfor i in range(1, X_train_poly.shape[1]+1, 1):\n    # Select i features\n    select = SelectKBest(score_func=chi2, k=i)\n    select.fit(X_train_poly, y_train)\n    X_train_poly_selected = select.transform(X_train_poly)\n\n    # Modelo con características i seleccionadas\n    logreg.fit(X_train_poly_selected, y_train)\n    scores = cross_val_score(logreg, X_train_poly_selected, y_train, cv=10)\n    print('CV accuracy (number of features = %i): %.3f +/- %.3f' % (i, \n                                                                     np.mean(scores), \n                                                                     np.std(scores)))\n    \n    # Guardar resultados si la mejor puntuación\n    if np.mean(scores) > highest_score:\n        highest_score = np.mean(scores)\n        std = np.std(scores)\n        k_features_highest_score = i\n    elif np.mean(scores) == highest_score:\n        if np.std(scores) < std:\n            highest_score = np.mean(scores)\n            std = np.std(scores)\n            k_features_highest_score = i\n        \n# Imprimir el número de características\nprint('Number of features when highest score: %i' % k_features_highest_score)","metadata":{"_cell_guid":"f745b00f-d0f7-4944-a276-c62035876bca","_uuid":"cecdfd015c0359c6869284f1ddaf34c1cc48499e","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{"_cell_guid":"5da908fb-dc93-444a-b5c2-30f18e43c9b6","_uuid":"c084561c2604dbed0b19f1e8a7849f2b646b62e4"}},{"cell_type":"markdown","source":"# 3. Modelo de unicornio","metadata":{"_cell_guid":"654739a7-2abd-4570-bc4f-7086871ad22b","_uuid":"8e77b5aeff28b08030a3c10b7a8440a710b8f43a"}},{"cell_type":"markdown","source":"Las empresas emergentes utilizan el término \"unicornio\" para describir una empresa emergente valorada en mil millones de dólares o más. Independientemente de si se trata de [mil millones europeos o estadounidenses] (http://mathforum.org/library/drmath/view/52579.html), mil millones es un número grande. De hecho, es tan grande y raro que cuando encontramos startups con tanto valor, las asociamos a esas criaturas míticas que son los unicornios.\n\nHemos recorrido un largo camino desde que empezamos a resolver este problema. Desde nuestro comienzo con un modelo lean, hemos estado escalando nuestra puesta en marcha: imputamos datos faltantes, realizamos un análisis exploratorio de datos y extrajimos características. También tuvimos que lidiar con bromas terribles sobre el Titanic que tardan un tiempo en asimilarse.\n\nAhora es el momento de convertir todo este trabajo en un modelo de alta precisión, nuestro modelo 'unicornio'.","metadata":{"_cell_guid":"9f9fa0dd-5785-4716-9abf-43a3531b4bbe","_uuid":"c36a40b44a19385ec7127eb0d8a489341f0308b0"}},{"cell_type":"markdown","source":"## 3.1. Ajuste el modelo para la mejor combinación de características","metadata":{"_cell_guid":"d7f03e62-347d-4101-832c-b4e7f582b24d","_uuid":"4e584e021e75498b336fc906044f160d1afbfe91"}},{"cell_type":"code","source":"# Seleccionar características\nselect = SelectKBest(score_func=chi2, k=k_features_highest_score)\nselect.fit(X_train_poly, y_train)\nX_train_poly_selected = select.transform(X_train_poly)","metadata":{"_cell_guid":"f879ef7f-0e75-4fcc-8038-e7287412ddc9","_uuid":"c3948099bfcae0640f81daea45d3c13a1169f5f6","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Modelo de ajuste\nlogreg = LogisticRegression(C=1)\nlogreg.fit(X_train_poly_selected, y_train)","metadata":{"_cell_guid":"792f9ec4-2d7f-4a48-9386-4ff5cfb0fc49","_uuid":"6be7282799d01d81ebc3c3e4470fa3394895ae7f","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rendimiento del modelo\nscores = cross_val_score(logreg, X_train_poly_selected, y_train, cv=10)\nprint('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))","metadata":{"_cell_guid":"c7a2da1a-0bf9-4f87-ab9d-68eb8c6ab29a","_uuid":"2ea9009a51d43e3a05cb2052d053d1b80aef521f","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2. Curva de aprendizaje","metadata":{"_cell_guid":"c312a999-7f25-43a4-8a99-ab0023775cf2","_uuid":"9d88e9b61cc80315df7d0e25ae7dfd69dd0e5f6c"}},{"cell_type":"code","source":"# Trazar curvas de aprendizaje\ntitle = \"Learning Curves (Logistic Regression)\"\ncv = 10\nplot_learning_curve(logreg, title, X_train_poly_selected, \n                    y_train, ylim=(0.7, 1.01), cv=cv, n_jobs=1);","metadata":{"_cell_guid":"8b7a4e42-2a66-4858-908e-736ed1ec0413","_uuid":"04102938ca057efa337a976e43a4327a9391b42c","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sin signos de sobreajuste o desajuste.","metadata":{"_cell_guid":"08281286-a20f-46f6-aefa-1bf2878548b2","_uuid":"a124faac987e2def59774489b290d33742132d23"}},{"cell_type":"markdown","source":"## 3.3. Validation curve","metadata":{"_cell_guid":"20892bbf-6f2a-4016-927c-3c2de71ce91c","_uuid":"e32ca6dd7ec5aa3c3f22e4f57453f94e80cef600"}},{"cell_type":"code","source":"# Trazar la curva de validación\ntitle = 'Validation Curve (Logistic Regression)'\nparam_name = 'C'\nparam_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0] \ncv = 10\nplot_validation_curve(estimator=logreg, title=title, X=X_train_poly_selected, y=y_train, \n                      param_name=param_name, ylim=(0.5, 1.01), param_range=param_range);","metadata":{"_cell_guid":"6e36decd-4199-4595-af5d-bdb1b4938691","_uuid":"0ea8f8a43216ca7f2e8b38e21c7bd0b7167dd9a1","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Usamos C=1, que está al borde del sobreajuste. En cuanto al underfitting, no hay indicios de ello ya que el modelo funciona bien.","metadata":{"_cell_guid":"98ecbba5-630b-4c53-8712-8f32e3309d15","_uuid":"ec47cd6b07de2a2829701f027c06ed9408dad613"}},{"cell_type":"markdown","source":"## 3.4. Enviar predicciones","metadata":{"_cell_guid":"47d8e146-c9f9-458c-ab7f-fcd9bc521642","_uuid":"b74faafa48d1060ec553204dd17dd690a127f6fa"}},{"cell_type":"code","source":"# Obtener conjunto de datos de prueba\ndf = pd.read_csv('../input/test.csv')\ndf_raw = df.copy()","metadata":{"_cell_guid":"a02b92b3-c7f6-4a1e-81fa-562b410b24ec","_uuid":"afca3da89dbbfe32c939155df80c8aacdecbd4cb","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transformar conjunto de datos (basado en el Capítulo 2)\n\n## 2.2\ndf['FamilySize'] = df['SibSp'] + df['Parch']\ndf.drop('SibSp',axis=1,inplace=True)\ndf.drop('Parch',axis=1,inplace=True)\ndf.drop(['Name','Ticket','Cabin'], axis=1, inplace=True)\n\ndf['Title']=0\nfor i in df:\n    df['Title']=df_raw['Name'].str.extract('([A-Za-z]+)\\.', expand=False)  \ndf_raw['Title'] = df['Title']  \nmeans = df_raw.groupby('Title')['Age'].mean()\nmap_means = means.to_dict()\nidx_nan_age = df.loc[np.isnan(df['Age'])].index\ndf.loc[idx_nan_age, 'Age'] = df['Title'].loc[idx_nan_age].map(map_means)\ndf['Title'] = df['Title'].map(titles_dict)\ndf['Title'] = pd.Categorical(df['Title'])\n\ndf['Imputed'] = 0\ndf.at[idx_nan_age.values, 'Imputed'] = 1\n\ndf['Age'] = pd.cut(df['Age'], bins=[0, 12, 50, 200], labels=['Child','Adult','Elder'])\n\n## 2.3\npassenger_id = df['PassengerId'].values\ndf.drop('PassengerId', axis=1, inplace=True)\ndf['Embarked'] = pd.Categorical(df['Embarked'])\ndf['Pclass'] = pd.Categorical(df['Pclass'])\ndf = pd.get_dummies(df, drop_first=1)\n\ndf = df.fillna(df.mean())  # Falta un valor en 'Tarifa'\n\nX = df[df.loc[:, df.columns != 'Survived'].columns]\n\nX_transformed = X.copy()\nX_transformed['Fare'] = boxcox(X_transformed['Fare'] + 1)[0]\n\nscaler = MinMaxScaler()\nX_transformed_scaled = scaler.fit_transform(X_transformed)\n\npoly = PolynomialFeatures(degree=2).fit(X_transformed)\nX_poly = poly.transform(X_transformed_scaled)\n\nX_poly_selected = select.transform(X_poly)","metadata":{"_cell_guid":"49882ff1-315f-437b-ad03-c8a162e7c2bd","_uuid":"130076a891c5562a2369e03fd204d0004ba6aeb7","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hacer predicciones\npredictions = logreg.predict(X_poly_selected)","metadata":{"_cell_guid":"ed44a7c4-8d1b-4422-b0af-275eb75b5568","_uuid":"fa23074a440de2b0dbc93f8554422236a8222c3d","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generar archivo de envío\nsubmission = pd.DataFrame({ 'PassengerId': passenger_id,\n                            'Survived': predictions})\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"_cell_guid":"c696b30e-a35e-4dfc-8680-43285d1f8e0e","_uuid":"11e686460e8dc5237d2af9929e1bc9823ccef780","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{"_cell_guid":"9ff220cc-e58a-47b3-a1b9-b743a55b989f","_uuid":"990ffad8e44e5bd89e466b8edebdc204419b4411"}},{"cell_type":"markdown","source":"# 4. Conclusión","metadata":{"_cell_guid":"f16812aa-aecb-4214-a597-acef92740713","_uuid":"22d9a89fb1e536ae012e6be15530dc74d70cc2b5"}},{"cell_type":"markdown","source":"Como [Halevy et al. (2009)](https://static.googleusercontent.com/media/research.google.com/pt-PT//pubs/archive/35179.pdf) señaló que \"invariablemente, los modelos simples y una gran cantidad de datos triunfan sobre los más elaborados modelos basados en menos datos.' [Monica Rogati](https://youtu.be/F7iopLnhDik) agregó que \"mejores datos vencen a más datos\". Basado en estos principios, el objetivo de este estudio fue mejorar la calidad de los datos a través del análisis exploratorio de datos y la extracción de características. No usamos un algoritmo inteligente, pero exploramos técnicas inteligentes para mejorar nuestros datos.\n​\nMi expectativa es que después de leer este kernel, pueda comenzar a compilar un libro de recetas de técnicas en análisis exploratorio de datos y extracción de características. Estas técnicas lo ayudarán a obtener confianza en sus datos y a comprometerse con cualquier problema de ciencia de datos. Además, cuanto más utilice y perfeccione estas técnicas, más desarrollará su intuición para resolver problemas.\n​\nAhora es tu turno. Haz tuyo este trabajo. Seleccione una parte de este núcleo y juegue con él. ¿Por qué no probar un proceso de selección de características diferente? ¿O qué hay de aplicar un método de imputación diferente? Hay cien formas diferentes de [robar esta obra como un artista](https://youtu.be/oww7oB9rjgw). Hazlo... Después de todo, todos los unicornios comenzaron con un MVP.","metadata":{"_cell_guid":"9f04b0b1-177d-4f91-acfb-66a17d9d0876","_uuid":"3fd027961766c0243f3ca0ba5f9e8217e89e0377"}},{"cell_type":"markdown","source":"---","metadata":{"_cell_guid":"b07b837e-b626-494f-a33f-43b1e64e53f2","_uuid":"b6b196bff422c1ce06c690832eaa3fdd44d21907"}},{"cell_type":"markdown","source":"# También podría gustarte\n* [Mi blog](http://pmarcelino.com/)\n* [Exploración completa de datos en Python](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python)","metadata":{"_cell_guid":"c18f18df-c1c3-45a1-a434-b57bbec3d2f6","_uuid":"4b4ab9c9d9848a991af8802ea4d1997e7013bf8e"}},{"cell_type":"markdown","source":"# Referencias\n\n**Libros**\n\n* [Ries, E., 2011. The Lean Startup: cómo los empresarios de hoy usan la innovación continua para crear negocios radicalmente exitosos] (https://amzn.to/2JsIMH3)\n* [Hair, J.F., Black, W.C., Babin, B.J., Anderson, R.E. y Tatham, R.L., 2013. Análisis de datos multivariante](https://amzn.to/2JtC1Vm)\n* [Asefeso, A., Lund, S.B., Parry, H., 2014. Mantenga sus ojos en el horizonte: Lecciones de negocios del insumergible Titanic](https://amzn.to/2JwpsIT)\n\n**Documentos**\n\n* [Heaton, J., 2016. Un análisis empírico de la ingeniería de características para el modelado predictivo. En SoutheastCon, 2016 (págs. 1-6). IEEE.](https://arxiv.org/pdf/1701.07852.pdf)\n* [Recuadro, G.E. y Cox, D.R., 1964. Un análisis de las transformaciones. Revista de la Real Sociedad Estadística. Serie B (Metodológica), pp.211-252.](https://www.nuffield.ox.ac.uk/users/cox/cox72.pdf)\n* [Guyon, I. y Elisseeff, A., 2003. Una introducción a la selección de características y variables. Diario de investigación de aprendizaje automático, 3 (marzo), pp.1157-1182.] (http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf)\n* [Halevy, A., Norvig, P. y Pereira, F., 2009. La irrazonable efectividad de los datos. IEEE Intelligent Systems, 24(2), pp.8-12.](https://static.googleusercontent.com/media/research.google.com/pt-PT//pubs/archive/35179.pdf)\n\n**Vídeos**\n\n* [Rogati, M., 2012. El modelo y el tren descarrilado: un procedimiento de datos de entrenamiento. Estratos de O'Reilly.](https://youtu.be/F7iopLnhDik)\n* [Kleon, A., 2012. Roba como un artista. Charlas TEDx.](https://youtu.be/oww7oB9rjgw)","metadata":{"_cell_guid":"e391d0f1-6f0b-49cb-8d05-cbe9617625c8","_uuid":"b4ba33c97b17c58e9c2d504c6a23c95dac20f23c"}}]}